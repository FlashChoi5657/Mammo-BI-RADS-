import torch
import torch.nn as nn
import torch.nn.functional as F
import os
from sklearn.metrics import accuracy_score, f1_score, recall_score
import timm

# ===== ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜ =====
def convert_to_grayscale(model):
    for name, module in model.named_children():
        if isinstance(module, nn.Conv2d) and module.in_channels == 3:
            new_conv = nn.Conv2d(4, module.out_channels, module.kernel_size,
                                  module.stride, module.padding, bias=module.bias is not None)
            with torch.no_grad():
                avg_weight = module.weight.data.mean(dim=1, keepdim=True)
                new_conv.weight.data = avg_weight.repeat(1, 4, 1, 1)
                if module.bias is not None:
                    new_conv.bias.data = module.bias.data
            setattr(model, name, new_conv)
            print(f"âœ… Replaced Conv2d in {name}")
            break
        else:
            convert_to_grayscale(module)

def get_timm_model(name, num_classes=4, gray_scale=False):
    try:
        model = timm.create_model(name, pretrained=True)
    except:
        model = timm.create_model(name, pretrained=False)
        print(f"âŒ Failed to load {name}")
    model.reset_classifier(num_classes)

    if gray_scale:
        convert_to_grayscale(model)

    total = sum(p.numel() for p in model.parameters())
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ“¦ ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total:,}")
    print(f"ğŸ¯ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable:,}")
    return model

class EarlyStoppingTopK:
    def __init__(self, patience=5, save_dir='checkpoints', top_k=3):
        """
        ìƒìœ„ Kê°œ ëª¨ë¸ì„ ì €ì¥í•˜ê³ , ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œí•˜ëŠ” ìœ í‹¸ë¦¬í‹°

        Parameters:
        - patience (int): ê°œì„ ë˜ì§€ ì•Šì•„ë„ ê¸°ë‹¤ë¦´ ìµœëŒ€ epoch ìˆ˜
        - save_dir (str): ëª¨ë¸ íŒŒì¼ ì €ì¥ í´ë”
        - top_k (int): ì €ì¥í•  ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìˆ˜
        """
        self.patience = patience                # ìµœëŒ€ í—ˆìš© ì •ì²´ epoch ìˆ˜
        self.save_dir = save_dir                # ëª¨ë¸ ì €ì¥ í´ë”
        self.top_k = top_k                      # ì €ì¥í•  top-k ëª¨ë¸ ê°œìˆ˜
        self.counter = 0                        # ì—°ì†ìœ¼ë¡œ ê°œì„ ë˜ì§€ ì•Šì€ epoch ìˆ˜
        self.early_stop = False                 # ì¤‘ë‹¨ ì—¬ë¶€
        self.best = []                          # [(val_loss, index)] í˜•íƒœë¡œ ì €ì¥
        os.makedirs(save_dir, exist_ok=True)    # ì €ì¥ ê²½ë¡œ ìƒì„±

    def __call__(self, val_loss, model):
        """
        ë§¤ epochë§ˆë‹¤ í˜¸ì¶œí•˜ì—¬ val_lossë¥¼ í‰ê°€í•˜ê³  ëª¨ë¸ì„ ì €ì¥ ë˜ëŠ” ì¤‘ë‹¨ íŒë‹¨

        Parameters:
        - val_loss (float): í˜„ì¬ validation loss
        - model (nn.Module): í˜„ì¬ ëª¨ë¸ ê°ì²´

        Returns:
        - early_stop (bool): í•™ìŠµì„ ë©ˆì¶°ì•¼ í•˜ëŠ”ì§€ ì—¬ë¶€
        """
        # ì¡°ê±´: ì•„ì§ top_k ë¯¸ë§Œì´ê±°ë‚˜, ê¸°ì¡´ top_k ì¤‘ ê°€ì¥ ì•ˆ ì¢‹ì€ lossë³´ë‹¤ ë” ì¢‹ì„ ë•Œ
        if len(self.best) < self.top_k or val_loss < self.best[-1][0]:
            # ì¸ë±ìŠ¤ ê²°ì • (ìƒˆë¡œ ì¶”ê°€ or ê°€ì¥ ë‚˜ìœ ê²ƒ ë®ì–´ì“°ê¸°)
            idx = len(self.best) if len(self.best) < self.top_k else self.top_k - 1
            # íŒŒì¼ ì €ì¥
            torch.save(model.state_dict(), os.path.join(self.save_dir, f"checkpoint_top{idx+1}.pt"))
            # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ê³  ì •ë ¬
            self.best.append((val_loss, idx))
            self.best.sort()  # val_loss ì˜¤ë¦„ì°¨ìˆœ
            self.best = self.best[:self.top_k]  # top_kë§Œ ìœ ì§€
            # ì´ë¦„ ì¬ì •ë ¬ (top1 ~ topK)
            for i, (loss, _) in enumerate(self.best):
                path = os.path.join(self.save_dir, f"checkpoint_top{i+1}.pt")
                torch.save(model.state_dict(), path)
            self.counter = 0  # ê°œì„ ë˜ì—ˆìœ¼ë¯€ë¡œ counter ì´ˆê¸°í™”
        else:
            self.counter += 1  # ê°œì„ ë˜ì§€ ì•ŠìŒ â†’ ì¹´ìš´í„° ì¦ê°€
            if self.counter >= self.patience:
                self.early_stop = True  # patience ì´ˆê³¼ ì‹œ ì¡°ê¸° ì¢…ë£Œ

        return self.early_stop

def ensemble_predict(models, dataloader, device, weights=None, voting='soft'):
    """
    ì•™ìƒë¸” ì˜ˆì¸¡ í•¨ìˆ˜ (DataLoader ì§€ì›, soft/hard voting + ì„±ëŠ¥ í‰ê°€ í¬í•¨)

    Parameters:
        models (list): PyTorch ë¶„ë¥˜ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
        dataloader (DataLoader): (images, labels) íŠœí”Œ ì œê³µ
        device (str): ì‹¤í–‰ ë””ë°”ì´ìŠ¤ (ì˜ˆ: 'cuda' ë˜ëŠ” 'cpu')
        weights (list or None): soft votingì—ì„œ ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ (Noneì´ë©´ í‰ê· )
        voting (str): 'soft' ë˜ëŠ” 'hard'

    Returns:
        all_preds (Tensor): ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ (N,)
        all_probs (Tensor or None): soft votingì¼ ê²½ìš° í‰ê·  í™•ë¥  ë¶„í¬ (N, num_classes)
        metrics (dict): ì •í™•ë„, F1, Recall
    """
    for model in models:
        model.eval()
        model.to(device)

    all_preds = []
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            if isinstance(batch, (list, tuple)):
                images, labels = batch
            else:
                raise ValueError("Dataloader must return (images, labels) tuple.")
            images, labels = images.to(device), labels.to(device)

            batch_probs = []
            batch_preds = []

            for model in models:
                output = model(images)              # (B, num_classes)
                prob = F.softmax(output, dim=1)     # (B, num_classes)
                batch_probs.append(prob)
                batch_preds.append(prob.argmax(dim=1))  # (B,)

            if voting == 'hard':
                votes = torch.stack(batch_preds, dim=0)  # (num_models, B)
                pred_class = torch.mode(votes, dim=0).values  # (B,)
                all_preds.append(pred_class)

            else:  # soft voting
                probs = torch.stack(batch_probs, dim=0)  # (num_models, B, num_classes)
                if weights:
                    weights_tensor = torch.tensor(weights, device=device).view(-1, 1, 1)
                    weighted_probs = probs * weights_tensor
                    avg_prob = weighted_probs.sum(dim=0) / weights_tensor.sum()
                else:
                    avg_prob = probs.mean(dim=0)  # (B, num_classes)

                pred_class = avg_prob.argmax(dim=1)  # (B,)
                all_preds.append(pred_class)
                all_probs.append(avg_prob)

            all_labels.append(labels)

    all_preds = torch.cat(all_preds, dim=0).cpu()
    all_labels = torch.cat(all_labels, dim=0).cpu()

    metrics = {
        'accuracy': accuracy_score(all_labels, all_preds),
        'f1_score': f1_score(all_labels, all_preds, average='macro'),
        'recall': recall_score(all_labels, all_preds, average='macro'),
    }
    print(metrics)

    if voting == 'soft':
        all_probs = torch.cat(all_probs, dim=0).cpu()
        return all_preds, all_probs, metrics
    else:
        return all_preds, None, metrics
